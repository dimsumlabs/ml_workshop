{
 "metadata": {
  "name": "",
  "signature": "sha256:652fc36728528e0363e3e8c9ffb69339120037d52790fa87e43dc7d6ba6ed50b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we need to tell this thing to generate figures inline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "rcParams['figure.figsize'] = 12, 8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the dataset into memory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "data = load_digits()\n",
      "X = data['data']\n",
      "n_sample, n_dim = X.shape\n",
      "print('number of samples', n_sample)\n",
      "print('number of dimensions', n_dim)\n",
      "y = data['target']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check out one feature vector. The feature vectors are basically flattened 8 x 8 arrays of (inverse) pixel intensities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_vector = X[0].reshape([8, 8])\n",
      "feature_vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imshow(feature_vector, cmap='gray', interpolation='nearest');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like a zero. Let's check"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load an utility to split this dataset into a training and test set. We'll train a model on the training set and then evaluate it on the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "train_test_split(X, y, test_size=0.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatter(Xt[:, 0], Xt[:, 1], c=y, s=50)\n",
      "colorbar(); # the ; supresses output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check how the first two principal components (the two eigenvectors of the covariance matrix which account for most of the variance in the feature matrix) look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['figure.figsize'] = 8, 4\n",
      "f = figure()\n",
      "ax = f.add_subplot('121')\n",
      "ax.imshow(pca.components_[0].reshape(8, 8), cmap='gray', interpolation='nearest')\n",
      "ax2 = f.add_subplot('122')\n",
      "ax2.imshow(pca.components_[1].reshape(8, 8), cmap='gray', interpolation='nearest');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- The first one has a lot of overlap with a \"3\" and a \"9\" and is very opposite a \"4\" (especially the left side of the \"4\").\n",
      "- The second one has high values in the middle and low values around that. Much like \"1\" or a \"7\" and very opposite of a \"0\".\n",
      "\n",
      "If you check the previous figure you see that this is also what the data says.\n",
      "\n",
      "- The first principal component (x axis) has the highest values for \"3\" and \"9\" but the lowest for \"4\".\n",
      "- The second principal component (y axis) has the lowest values for \"0\" and the highest values for \"1\" and \"7\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's finally train a classifier to see how well we can actually learn this dataset. First we'll split the data into a training and a validation (test) set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll import the logistic regression classifier which is one of the simplest classifier out there and fit it to the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "log_reg_model = LogisticRegression()\n",
      "log_reg_model.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll make predictions for the validation set and check how well we did"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = log_reg_model.predict(X_test)\n",
      "print('Classification accuracy', np.mean(y_pred == y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "svc_model = SVC()\n",
      "y_pred = svc_model.fit(X_train, y_train).predict(X_test)\n",
      "print('Classification accuracy', np.mean(y_pred == y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oops. This is pretty bad. I forgot to scale the data. Some machine learning algorithms require the data to be centered and have unite variance. SVM is one of those algorithms. This can be achieved by removing from the column mean from each element of ```X``` and then dividing each element by the columns standard deviation. Easy to do by hand but we would have to check against dividing by zero, so let's use sklearns utility ```StandardScaler```."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_train_t = scaler.fit_transform(X_train)\n",
      "X_test_t = scaler.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we fit and predict again with the scaled data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = svc_model.fit(X_train_t, y_train).predict(X_test_t)\n",
      "print('Classification accuracy', np.mean(y_pred == y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Yes](http://fc01.deviantart.net/fs71/i/2010/171/0/b/Fuck_Yea__in_HD_by_CrusierPL.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get an even better idea about the performance of our model we could repeat the training / test split a few times and average the results. Often this is done in 5 - 10 splits such that each sample is exactly once in the test set. This is called cross validation. It's also possible to combine transformations (like PCA or the StandardScaler) and a final estimator (like SVM) together into a pipeline.\n",
      "\n",
      "Let's try this too."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.pipeline import make_pipeline\n",
      "pipeline = make_pipeline(StandardScaler(), SVC())\n",
      "print('Cross validation score', np.mean(cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}