{
 "metadata": {
  "name": "",
  "signature": "sha256:3aef73ce7f3be33a7a06affab83112c610444dd65f1896f69864f29077a9a97b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're going to scrape some air quality data from http://www.aqhi.gov.hk/en.html.\n",
      "\n",
      "### Set up scrapy project\n",
      "First lets create a scrapy project skeleton. Navigate to the directory where you want to create the project and exetute,\n",
      "\n",
      "```\n",
      "scrapy startproject airq\n",
      "```\n",
      "this will create the following project structure\n",
      "```\n",
      "airq\n",
      "\u251c\u2500\u2500 airq\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 items.py\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 pipelines.py\n",
      "\u2502\u00a0\u00a0 \u251c\u2500\u2500 settings.py\n",
      "\u2502\u00a0\u00a0 \u2514\u2500\u2500 spiders\n",
      "\u2502\u00a0\u00a0     \u2514\u2500\u2500 __init__.py\n",
      "\u2514\u2500\u2500 scrapy.cfg\n",
      "```\n",
      "On a very high level (and AFAIK), scrapy does roughly the following when called with ```scrapy crawl spider_name```:\n",
      "\n",
      "    1. Create HTTP respose object based on the url(s) specified in the spider\n",
      "    2. Call the spiders 'parse' method on each response object\n",
      "    3. For each item return by the 'parse' method, call the specified Pipeline object(s)\n",
      "    \n",
      "for our simple example we need to populate,\n",
      "\n",
      "- ```items.py```, where we define the structure of our data\n",
      "- ```spiders/airq_spider.py```, where we tell our spider which URLs to go to and how to extract the data. The spider's 'parse' method (the main task), returns a list of 'items'. Each item is then passed through the pipelines.\n",
      "- ```pipelines.py```, where we decide where to store the extracted items (database, flat file, etc.)\n",
      "- ```settings.py```, where we tell scrapy what pipeline(s) to use\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, to start, lets define our item\n",
      "\n",
      "#### items.py\n",
      "```python\n",
      "from scrapy.item import Item, Field                                                \n",
      "                                                                                   \n",
      "class AirqItem(Item):                                                              \n",
      "    # define the fields for your item here like:                                   \n",
      "    station = Field()                                                              \n",
      "    air_quality = Field()                                                          \n",
      "    timestamp = Field() \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Extracting the Data\n",
      "Now the more tricky part, our spider. Below is what the file could look like. The parts where we extract the desired information from the website have been removed.\n",
      "\n",
      "#### spiders/airq_spider.py (you need to generate this file)\n",
      "```python\n",
      "from dateutil.parser import parse                                                  \n",
      "                                                                                   \n",
      "from scrapy import log # This module is useful for printing out debug information\n",
      "from scrapy.spider import Spider                                                   \n",
      "from scrapy.selector import Selector                                               \n",
      "                                                                                   \n",
      "from airq.items import AirqItem                                                    \n",
      "                                                                                   \n",
      "                                                                                   \n",
      "class AirqSpider(Spider):                                                          \n",
      "    name = 'air_quality'                                                           \n",
      "    allowed_domains = ['aqhi.gov.hk']                                              \n",
      "    start_urls = [                                                                 \n",
      "            'http://www.aqhi.gov.hk/en.html',                                      \n",
      "    ]                                                                              \n",
      "                                                                                   \n",
      "    def parse(self, response):                                                     \n",
      "        sel = Selector(response)                                                   \n",
      "        stations = # get the station names                                                      \n",
      "        values = # get the air quality index values                                                  \n",
      "        t = # get the current timestamp as string from the website                                                    \n",
      "        timestamp = parse(t)                                                       \n",
      "                                                                                   \n",
      "        items = []                                                                 \n",
      "        \n",
      "        # create items\n",
      "        for s, v in zip(stations, values):                                         \n",
      "            item = AirqItem()                                                      \n",
      "            item['station'] = s                                                    \n",
      "            item['air_quality'] = v                                                \n",
      "            item['timestamp'] = timestamp                                          \n",
      "            items.append(item)                                                     \n",
      "                                                                                   \n",
      "        self.log(\"Got {} items.\".format(len(items)))                               \n",
      "                                                                                   \n",
      "        return items                                                               \n",
      "                       \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we'd like to get the station names, the air quality index values and the current timestamp. At this stage we have to do some trial and error until we get the XPath selection statements right. The scrapy shell is useful for this, so, let's fire up a scrapy shell,\n",
      "\n",
      "```\n",
      "scrapy shell http://www.aqhi.gov.hk/en.html\n",
      "```\n",
      "scrapy has now already created a scrapy.selector.Selector object called 'sel' which we can use to poke around until we know how to extract the desired data from the website. For more info, refer to http://doc.scrapy.org/en/latest/topics/shell.html\n",
      "\n",
      "The scrapy shell pre-loads some objects into the ipython shell (which in turn is an enhanced python shell).\n",
      "\n",
      "So we'll hack around in the scrapy shell until we find the appropriate XPath statements and then put we'll those into the scrapy spider file (```airq_spider.py```)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the scrapy shell there should be a Selector object called 'sel'. We can use the sel.xpath method to extract the desired data with HTML XPath statements. The scrapy selector documentation is here, http://doc.scrapy.org/en/0.7/topics/selectors.html\n",
      "\n",
      "I don't know much about the XPath (or HTML really) language myself, what you may find useful are the following,\n",
      "\n",
      "    1. (In chromium) right-click on desired parts of the website and the click 'Inspect Element'\n",
      "    2. Look at the website's source code\n",
      "    3. There is a XPath helper chrome extension linked below\n",
      "\n",
      "[XPath helper chrome extension](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
      "\n",
      "Now dive into the scrapy shell and start finding the precious data.\n",
      "\n",
      "To get started, you can select all elements with 'td' tags with\n",
      "```python\n",
      "sel.xpath('//td')\n",
      "```\n",
      "To get select the text in each of those elements\n",
      "```python\n",
      "sel.xpath('//td/text()')\n",
      "```\n",
      "To extract the text\n",
      "```python\n",
      "sel.xpath('//td/text()').extract()\n",
      "```\n",
      "to only match the class (\"tblCurrAQHI_tdName\") in which the station names are stored\n",
      "```python\n",
      "sel.xpath('//td/[@class=\"tblCurrAQHI_tdName\"]')\n",
      "```\n",
      "you can also chain different tags in the hierarchy, for instance\n",
      "```python\n",
      "sel.xpath('//td/[@class=\"tblCurrAQHI_tdName\"]/a')\n",
      "```\n",
      "Finally, one way to get the station names is\n",
      "```python\n",
      "sel.xpath('//td[@class=\"tblCurrAQHI_tdName\"]/a/text()').extract()\n",
      "```\n",
      "In some cases you may need to do some additional cleaning up with python string operations or regex or whatever.\n",
      "\n",
      "I encourage everyone to try to get the air quality index values and the timestamp yourself.\n",
      "\n",
      "If you're fed up, the follwing few lines extract the data we want."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```python\n",
      "stations = sel.xpath('//td[@class=\"tblCurrAQHI_tdName\"]/a/text()').extract()\n",
      "\n",
      "values = [int(x) for x in \n",
      "          sel.xpath('//td[@class=\"tblCurrAQHI_tdBand notSurrogate\"]/text()').extract()\n",
      "          if x.isdigit()]\n",
      "\n",
      "t = sel.xpath('//tr[@class=\"tblCurrAQHI_trHeader\"]/td/text()').extract()[0]\n",
      "from dateutil import parser\n",
      "timestamp = parser.parse(t)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Storing the Data\n",
      "In this example we're gonna run a local mysql server and put some data into it from a scrapy Pipeline object.\n",
      "\n",
      "Fire up the mysql server, on ubuntu that's something like\n",
      "```bash\n",
      "sudo /etc/init.d/mysql start\n",
      "```\n",
      "The following script will create a database \"Scrapy\" a table \"AirQuality\" and a user \"scrapy\".\n",
      "\n",
      "```mysql\n",
      "CREATE DATABASE IF NOT EXISTS Scrapy;                                              \n",
      "CREATE TABLE IF NOT EXISTS Scrapy.AirQuality (                                     \n",
      "    station VARCHAR(32),                                                           \n",
      "    timestamp DATETIME,                                                            \n",
      "    air_quality INT,                                                               \n",
      "    PRIMARY KEY (station, timestamp));                                             \n",
      "GRANT ALL ON Scrapy.AirQuality TO 'scrapy'@'localhost' IDENTIFIED BY 'some_password';\n",
      "FLUSH PRIVILEGES;  \n",
      "```\n",
      "if you save the script as mysql_setup.sql you can run it as follows\n",
      "```bash\n",
      "mysql --host localhost --user root < mysql_setup.sql\n",
      "```\n",
      "\n",
      "Now we edit ```pipelines.py```, the 'open_spider' and 'close_spider' methods are called only once for each run of the spider, the 'process_item' method is called once for each item.\n",
      "####pipelines.py\n",
      "```python\n",
      "# Define your item pipelines here                                                  \n",
      "#                                                                                  \n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting                  \n",
      "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html                   \n",
      "from MySQLdb import connect                                                        \n",
      "from scrapy import log                                                             \n",
      "                                                                                   \n",
      "HOST = 'localhost'                                                                 \n",
      "USER = 'scrapy'                                                                    \n",
      "PASSWD = 'some_password'                                                           \n",
      "DB = 'Scrapy'                                                                      \n",
      "                                                                                   \n",
      "                                                                                   \n",
      "class SQL(object):                                                                 \n",
      "    def open_spider(self, spider):                                                 \n",
      "        self.dbcon = connect(host=HOST,                                            \n",
      "                             user=USER,                                            \n",
      "                             passwd=PASSWD,                                        \n",
      "                             db=DB)                                                \n",
      "        self.cursor = self.dbcon.cursor()                                          \n",
      "                                                                                   \n",
      "    def process_item(self, item, spider):                                          \n",
      "        self.cursor.execute(\"INSERT INTO AirQuality (station, timestamp, \"         \n",
      "                \"air_quality) VALUES (%s, %s, %s)\", (item['station'],              \n",
      "                                                     item['timestamp'],            \n",
      "                                                     item['air_quality']))         \n",
      "        return item                                                                \n",
      "                                                                                   \n",
      "    def close_spider(self, spider):                                                \n",
      "        self.dbcon.commit()      \n",
      "```\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's it, almost, we still need to specify which pipelines to use in what order in ```settings.py```.\n",
      "\n",
      "####settings.py\n",
      "```python\n",
      "# Scrapy settings for airq project                                                 \n",
      "#                                                                                  \n",
      "# For simplicity, this file contains only the most important settings by           \n",
      "# default. All the other settings are documented here:                             \n",
      "#                                                                                  \n",
      "#     http://doc.scrapy.org/en/latest/topics/settings.html                         \n",
      "#                                                                                  \n",
      "                                                                                   \n",
      "BOT_NAME = 'airq'                                                                  \n",
      "                                                                                   \n",
      "SPIDER_MODULES = ['airq.spiders']                                                  \n",
      "NEWSPIDER_MODULE = 'airq.spiders'                                                  \n",
      "                                                                                   \n",
      "ITEM_PIPELINES = {'airq.pipelines.SQL': 100}                                       \n",
      "                                                                                   \n",
      "# LOG_FILE = 'scrapy.log'                                                          \n",
      "                                                                                   \n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = 'airq (+http://www.yourdomain.com)'   \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we may or may not have everything in place and can run the spider with\n",
      "```\n",
      "scrapy crawl air_quality\n",
      "```\n",
      "from the root directory of the project (the directory that contains the ```scrapy.cfg``` file).\n",
      "\n",
      "If you'd like to take this further, you may want to read on here http://scrapyd.readthedocs.org/en/latest/#deploying-your-project or simply run a cron job every so often."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}